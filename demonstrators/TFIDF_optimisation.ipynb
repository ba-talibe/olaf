{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oumar/Bureau/ontology-learning/env/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import logging\n",
    "import pandas as pd\n",
    "from olaf import Pipeline\n",
    "from typing import Set, List\n",
    "from olaf.commons.logging_config import logger\n",
    "from olaf.data_container import CandidateTerm, Relation, Concept\n",
    "from olaf.data_container.knowledge_representation_schema import KnowledgeRepresentation\n",
    "from olaf.pipeline.pipeline_component.term_extraction import (\n",
    "    POSTermExtraction,\n",
    "    TFIDFTermExtraction,\n",
    "    ManualCandidateTermExtraction\n",
    "    )\n",
    "from olaf.pipeline.pipeline_component.concept_relation_extraction import (\n",
    "    CTsToConceptExtraction,\n",
    "    CTsToRelationExtraction,\n",
    "    SynonymRelationExtraction,\n",
    "    SynonymConceptExtraction,\n",
    "    AgglomerativeClusteringRelationExtraction,\n",
    "    AgglomerativeClusteringConceptExtraction\n",
    ")\n",
    "from olaf.commons.spacy_processing_tools import is_not_punct, is_not_stopword, select_on_pos\n",
    "\n",
    "from olaf.pipeline.pipeline_component.candidate_term_enrichment import SemanticBasedEnrichment\n",
    "\n",
    "from olaf.repository.corpus_loader.text_corpus_loader import TextCorpusLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import concurrent.futures\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from matplotlib_venn import venn2, venn3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = \"GC10-DET_doc.txt\"\n",
    "corpus_loader = TextCorpusLoader(corpus_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select and create relevent concepts from the corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Punching, Welding line, Crescent Gap, Water spot, Oil spot, Silk spot, Inclusion, Rolled pit, Crease, Waist folding, metal surface defect, mechanical failure, drying, mechanical lubricant, temperature, pressure, work roll damage, tension roll damage, local yield, low-carbon]\n"
     ]
    }
   ],
   "source": [
    "expected_concepts = []\n",
    "with open(\"concepts.txt\", 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    expected_concepts = [concept.rstrip(\"\\n\") for concept in lines]\n",
    "    expected_concepts = [Concept(concept) for concept in expected_concepts]\n",
    "    f.close()\n",
    "\n",
    "print(expected_concepts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing concept ratio function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "\n",
    "def is_similar(concept_a : str, concept_b: str, nlp =nlp, threshold=.8):\n",
    "    vector_a, vector_b = nlp(concept_a).vector, nlp(concept_b).vector\n",
    "    return cosine_similarity([vector_a], [vector_b]) > threshold\n",
    "\n",
    "def is_equal(concept_a : str, concept_b: str):\n",
    "    return concept_a.lower() == concept_b.lower()\n",
    "\n",
    "def hg_lm_similaritiry(concept_a : str, concept_b: str, model=SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\"), threshold=.8):\n",
    "    embedding_a, embedding_b = model.encode(concept_a), model.encode(concept_b)\n",
    "    return util.pytorch_cos_sim(embedding_a, embedding_b) > threshold\n",
    "\n",
    "def get_concept_ratio(pipeline : Pipeline, expected_concepts : List[Concept], comparator = hg_lm_similaritiry, comparator_args:dict={}) -> tuple:\n",
    "    \"\"\"\n",
    "    Calculate the ratio of expected and unexpected concepts in a given pipeline.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pipeline : Pipeline\n",
    "        The pipeline object containing concepts.\n",
    "    expected_concepts : List[Concept]\n",
    "        A list of expected concepts.kwargs\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[float, float]: A tuple containing:\n",
    "        The percentage of expected concepts found in the pipeline.\n",
    "        The percentage of unexpected concepts in the pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    found_concepts = pipeline.kr.concepts\n",
    "    if len(found_concepts := pipeline.kr.concepts) == 0:\n",
    "        pipeline.run()\n",
    "        if len(found_concepts := pipeline.kr.concepts) == 0:\n",
    "            return (0, 0, 0)\n",
    "\n",
    "    found_concepts = [found_concept.label for found_concept in found_concepts]\n",
    "    expected_concepts = [expected_concept.label for expected_concept in expected_concepts]\n",
    "    expected_concept_occ = 0\n",
    "    for expected_concept in expected_concepts:\n",
    "        for found_concept in found_concepts:\n",
    "            if comparator(expected_concept, found_concept, **comparator_args):\n",
    "                expected_concept_occ += 1\n",
    "                break \n",
    "\n",
    "    precision = expected_concept_occ/len(expected_concepts)\n",
    "    recall = expected_concept_occ/len(found_concepts)\n",
    "    f1 = 2*(precision * recall)/(precision+recall)\n",
    "    return (precision, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from olaf.pipeline.pipeline_component.term_extraction.manual_candidate_terms import (\n",
    "    ManualCandidateTermExtraction,\n",
    ")\n",
    "from olaf.pipeline.pipeline_component.concept_relation_extraction import (\n",
    "    CTsToConceptExtraction,\n",
    "    CTsToRelationExtraction\n",
    ")\n",
    "\n",
    "# concept extraction component\n",
    "concepts = [\n",
    "    \"defect type\",\n",
    "    \"steel strip surface\",\n",
    "    \"punching\",\n",
    "    \"mechanical failure\",\n",
    "    \"welding line\",\n",
    "    \"coil\",\n",
    "    \"weld line\",\n",
    "    \"crescent gap\",\n",
    "    \"cutting\",\n",
    "    \"water spot\",\n",
    "    \"drying\",\n",
    "    \"oil spot\",\n",
    "    \"mechanical lubricant\",\n",
    "    \"silk spot\",\n",
    "    \"plaque\",\n",
    "    \"strip surface\",\n",
    "    \"roller\",\n",
    "    \"pressure\",\n",
    "    \"inclusion\",\n",
    "    \"metal surface\",\n",
    "    \"spots\",\n",
    "    \"fish scale shape\",\n",
    "    \"block irregular distribution\",\n",
    "    \"rolled pit\",\n",
    "    \"bulges\",\n",
    "    \"pits\",\n",
    "    \"steel plate\",\n",
    "    \"work roll\",\n",
    "    \"tension roll\",\n",
    "    \"damage\",\n",
    "    \"crease\",\n",
    "    \"fold\",\n",
    "    \"uncoiling process\",\n",
    "    \"waist folding\",\n",
    "    \"deformation\",\n",
    "    \"low-carbon\"\n",
    "]\n",
    "\n",
    "relations = [\n",
    "    \"described\",\n",
    "    \"explaining\",\n",
    "    \"appears\",\n",
    "    \"leads\",\n",
    "    \"resulting\",\n",
    "    \"changed\",\n",
    "    \"produced\",\n",
    "    \"drying\",\n",
    "    \"caused\",\n",
    "    \"affect\",\n",
    "    \"appearing\",\n",
    "    \"lies\",\n",
    "    \"distributed\",\n",
    "    \"accompanied\",\n",
    "    \"showing\",\n",
    "    \"pressed\",\n",
    "    \"occurred\",\n",
    "    \"circumvented\",\n",
    "    \"detected\",\n",
    "    \"tracked\",\n",
    "    \"results\",\n",
    "    \"like\",\n",
    "    \"mainly\",\n",
    "    \"uncoiling\"\n",
    "]\n",
    "\n",
    "ct_concept_label = { concept : {concept} for concept in concepts}\n",
    "\n",
    "manuel_concept_extraction = ManualCandidateTermExtraction(\n",
    "    ct_label_strings_map=ct_concept_label\n",
    ")\n",
    "\n",
    "concept_extraction = CTsToConceptExtraction(\n",
    ")\n",
    "# concept extraction component\n",
    "\n",
    "\n",
    "\n",
    "relation_extraction = CTsToRelationExtraction()\n",
    "pipelines = []\n",
    "pipelines.append(\n",
    "    Pipeline(\n",
    "        spacy_model=nlp,\n",
    "        pipeline_components=[\n",
    "            manuel_concept_extraction,\n",
    "            concept_extraction,\n",
    "        ],\n",
    "        corpus_loader=corpus_loader\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8, 0.45714285714285713, 0.5818181818181818)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_pipeline = pipelines[-1]\n",
    "current_pipeline.run()\n",
    "\n",
    "get_concept_ratio(current_pipeline, expected_concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Punching : punching \n",
      "Welding line : weld line \n",
      "Crescent Gap : crescent gap \n",
      "Water spot : water spot \n",
      "Oil spot : oil spot \n",
      "Silk spot : silk spot \n",
      "Inclusion : inclusion \n",
      "Rolled pit : rolled pit \n",
      "Crease : crease \n",
      "Waist folding : waist folding \n",
      "metal surface defect : \n",
      "mechanical failure : mechanical failure \n",
      "drying : drying \n",
      "mechanical lubricant : mechanical lubricant \n",
      "temperature : \n",
      "pressure : pressure \n",
      "work roll damage : \n",
      "tension roll damage : tension roll \n",
      "local yield : \n",
      "low-carbon : low-carbon "
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8, 0.45714285714285713, 0.5818181818181818)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def debug_get_concept_ratio(pipeline : Pipeline, expected_concepts : List[Concept], comparator = hg_lm_similaritiry, comparator_args:dict={}) -> tuple:\n",
    "    \"\"\"\n",
    "    Calculate the ratio of expected and unexpected concepts in a given pipeline.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pipeline : Pipeline\n",
    "        The pipeline object containing concepts.\n",
    "    expected_concepts : List[Concept]\n",
    "        A list of expected concepts.kwargs\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[float, float]: A tuple containing:\n",
    "        The percentage of expected concepts found in the pipeline.\n",
    "        The percentage of unexpected concepts in the pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    found_concepts = pipeline.kr.concepts\n",
    "    if len(found_concepts := pipeline.kr.concepts) == 0:\n",
    "        pipeline.run()\n",
    "        if len(found_concepts := pipeline.kr.concepts) == 0:\n",
    "            return (0, 0, 0)\n",
    "\n",
    "    found_concepts = [found_concept.label for found_concept in found_concepts]\n",
    "    expected_concepts = [expected_concept.label for expected_concept in expected_concepts]\n",
    "    expected_concept_occ = 0\n",
    "    for expected_concept in expected_concepts:\n",
    "        print()\n",
    "        print(f\"{expected_concept} : \", end=\"\")\n",
    "        for found_concept in found_concepts:\n",
    "            if comparator(expected_concept, found_concept, **comparator_args):\n",
    "                print(f\"{found_concept} \", end=\"\")\n",
    "                expected_concept_occ += 1\n",
    "                break \n",
    "\n",
    "\n",
    "    precision = expected_concept_occ/len(expected_concepts)\n",
    "    recall = expected_concept_occ/len(found_concepts)\n",
    "    f1 = 2*(precision * recall)/(precision+recall)\n",
    "    return (precision, recall, f1)\n",
    "\n",
    "\n",
    "\n",
    "current_pipeline = pipelines[-1]\n",
    "debug_get_concept_ratio(current_pipeline, expected_concepts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimze the similarity threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8, 0.45714285714285713, 0.5818181818181818)\n",
      "(0.9, 0.5142857142857142, 0.6545454545454545)\n",
      "(0.9, 0.5142857142857142, 0.6545454545454545)\n",
      "(0.9, 0.5142857142857142, 0.6545454545454545)\n"
     ]
    }
   ],
   "source": [
    "print(get_concept_ratio(current_pipeline, expected_concepts)) # default threshold is 0.8\n",
    "print(get_concept_ratio(current_pipeline, expected_concepts, comparator_args={\"threshold\": 0.7}))\n",
    "print(get_concept_ratio(current_pipeline, expected_concepts, comparator_args={\"threshold\": 0.6}))\n",
    "print(get_concept_ratio(current_pipeline, expected_concepts, comparator_args={\"threshold\": 0.5}))\n",
    "\n",
    "comparator_args={\"threshold\": 0.7}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usefull function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_concept(kr: KnowledgeRepresentation) -> None:\n",
    "    print(\"Concepts in KR:\")\n",
    "    for concept in kr.concepts:\n",
    "        print(concept.label)\n",
    "\n",
    "\n",
    "def display_relation(kr: KnowledgeRepresentation) -> None:\n",
    "    print(\"Relations in KR:\")\n",
    "    for relation in kr.relations:\n",
    "        if (\n",
    "            relation.source_concept is not None\n",
    "            or relation.destination_concept is not None\n",
    "        ):\n",
    "            print(\n",
    "                (\n",
    "                    relation.source_concept.label,\n",
    "                    relation.label,\n",
    "                    relation.destination_concept.label,\n",
    "                )\n",
    "            )\n",
    "\n",
    "def describe_pipeline(pipeline: Pipeline) -> None:\n",
    "    print(pipeline.__class__.__name__)\n",
    "    for component in pipeline.pipeline_components:\n",
    "        print(f\"\\t {component.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimatimizing TFIDF pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from olaf.pipeline.pipeline_component.term_extraction import (\n",
    "    TFIDFTermExtraction\n",
    ")\n",
    "\n",
    "from olaf.pipeline.pipeline_component.concept_relation_extraction import (\n",
    "    CTsToConceptExtraction,\n",
    "    SynonymConceptExtraction,\n",
    "    AgglomerativeClusteringConceptExtraction\n",
    ")\n",
    "\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">CandidatToConcept</th>\n",
       "      <th colspan=\"3\" halign=\"left\">SynonymToConcept</th>\n",
       "      <th colspan=\"3\" halign=\"left\">AgglomerativeClustering</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Rappel</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Rappel</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Rappel</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.06</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.08</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.09</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     CandidatToConcept             SynonymToConcept              \\\n",
       "             Precision Rappel   F1        Precision Rappel   F1   \n",
       "0.01               NaN    NaN  NaN              NaN    NaN  NaN   \n",
       "0.02               NaN    NaN  NaN              NaN    NaN  NaN   \n",
       "0.03               NaN    NaN  NaN              NaN    NaN  NaN   \n",
       "0.04               NaN    NaN  NaN              NaN    NaN  NaN   \n",
       "0.05               NaN    NaN  NaN              NaN    NaN  NaN   \n",
       "0.06               NaN    NaN  NaN              NaN    NaN  NaN   \n",
       "0.07               NaN    NaN  NaN              NaN    NaN  NaN   \n",
       "0.08               NaN    NaN  NaN              NaN    NaN  NaN   \n",
       "0.09               NaN    NaN  NaN              NaN    NaN  NaN   \n",
       "0.10               NaN    NaN  NaN              NaN    NaN  NaN   \n",
       "\n",
       "     AgglomerativeClustering              \n",
       "                   Precision Rappel   F1  \n",
       "0.01                     NaN    NaN  NaN  \n",
       "0.02                     NaN    NaN  NaN  \n",
       "0.03                     NaN    NaN  NaN  \n",
       "0.04                     NaN    NaN  NaN  \n",
       "0.05                     NaN    NaN  NaN  \n",
       "0.06                     NaN    NaN  NaN  \n",
       "0.07                     NaN    NaN  NaN  \n",
       "0.08                     NaN    NaN  NaN  \n",
       "0.09                     NaN    NaN  NaN  \n",
       "0.10                     NaN    NaN  NaN  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concept_extraction_components = [\"CandidatToConcept\", \"SynonymToConcept\", \"AgglomerativeClustering\"]\n",
    "candidate_term_treshold = np.arange(0.01, 0.11, 0.01)\n",
    "results = pd.DataFrame(\n",
    "    index=concept_extraction_components,\n",
    "    )\n",
    "\n",
    "multi_index = pd.MultiIndex.from_product([\n",
    "   concept_extraction_components, \n",
    "    [\"Precision\", \"Rappel\", \"F1\"]\n",
    "    ])\n",
    "pipelines_scores = pd.DataFrame(index=candidate_term_treshold, columns=multi_index)\n",
    "\n",
    "pipelines_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_pipelines = []\n",
    "\n",
    "tfidf_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "\n",
    "def has_not_stopword(terms : Set[CandidateTerm], nlp=nlp,):\n",
    "    selected_term = []\n",
    "    for term in terms:\n",
    "        if is_not_stopword(nlp(term.label)[0]):\n",
    "            selected_term.append(term)\n",
    "    return set(selected_term)\n",
    "\n",
    "def has_not_punct(terms : Set[CandidateTerm], nlp=nlp,):\n",
    "    selected_term = []\n",
    "    for term in terms:\n",
    "        if all([is_not_punct(nlp(char)[0]) for char in term.label]):\n",
    "            selected_term.append(term)\n",
    "    return set(selected_term)\n",
    "\n",
    "\n",
    "bad_pos = [\"ADP\", \"PRON\", \"CCONJ\", \"PART\", \"DET\", \"INTJ\", \"SCONJ\", \"NUM\"]\n",
    "\n",
    "def has_not_bad_pos(terms : Set[CandidateTerm], nlp=nlp, pos_to_avoid: List[str]=bad_pos) -> Set[CandidateTerm]:\n",
    "    selected_term = []\n",
    "    for term in terms:\n",
    "        if not any([doc.pos_ in pos_to_avoid for doc in nlp(term.label)]):\n",
    "            selected_term.append(term)\n",
    "    return set(selected_term)\n",
    "\n",
    "def tidf_postprocessing(terms : Set[CandidateTerm], nlp=nlp) -> Set[CandidateTerm]:\n",
    "\n",
    "    # has_no_punct = lambda term: any([not is_not_punct(term[i]) for i in range(len(term))])\n",
    "    terms_doc = [nlp(term.label) for term in terms]\n",
    "    selected_term = [\n",
    "        term for term in terms\n",
    "        if  is_not_punct(nlp(term.label)[0])  \n",
    "            and is_not_stopword(nlp(term.label)[0])\n",
    "            and has_not_punct(nlp(term.label)[0])\n",
    "            and has_not_bad_pos(nlp(term.label)[0])\n",
    "        ]\n",
    "\n",
    "    return set(selected_term)\n",
    "\n",
    "def cts_post_processing(cts: set[CandidateTerm], bad_pos=bad_pos) -> set[CandidateTerm]:\n",
    "    \"\"\"Post processing on candidate terms.\n",
    "    Candidate terms with punctuation, stop words or verbs are removed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cts: set[CandidateTerm]\n",
    "        Set of candidate terms to filter.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    set[CandidateTerm]\n",
    "        The candidate terms validated.    \n",
    "    \"\"\"\n",
    "    existing_cts = []\n",
    "    new_cts = set()\n",
    "    for ct in cts:\n",
    "        keep = True\n",
    "        if len(ct.corpus_occurrences) > 0:\n",
    "            for co in ct.corpus_occurrences:\n",
    "                for token in co:\n",
    "                    if (not (is_not_punct(token)) or not (is_not_stopword(token)) or (token.pos_ in bad_pos)):\n",
    "                        keep = False\n",
    "                        break\n",
    "        else:\n",
    "            keep = False\n",
    "        if keep and ct.label not in existing_cts:\n",
    "            new_cts.add(ct)\n",
    "            existing_cts.append(ct.label)\n",
    "    return new_cts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF Term Extraction and Candidat To Concept Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oumar/Bureau/ontology-learning/env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ct_to_concept_pipelines = [\n",
    "    Pipeline(\n",
    "        spacy_model=nlp,\n",
    "        pipeline_components=[\n",
    "            TFIDFTermExtraction(\n",
    "                max_term_token_length=3,\n",
    "                candidate_term_threshold=threshold,\n",
    "                cts_post_processing_functions=[has_not_punct, has_not_bad_pos, has_not_stopword]\n",
    "            ),\n",
    "            CTsToConceptExtraction()\n",
    "        ],\n",
    "        corpus_loader=corpus_loader\n",
    "    ) for threshold in candidate_term_treshold]\n",
    "\n",
    "\n",
    "results = []\n",
    "# Utilisation de ThreadPoolExecutor pour exécuter les pipelines en parallèle\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = [\n",
    "        executor.submit(get_concept_ratio, pipeline, expected_concepts, hg_lm_similaritiry, comparator_args) \n",
    "        for pipeline in ct_to_concept_pipelines\n",
    "        ]\n",
    "    \n",
    "\n",
    "    # Attendre que tous les pipelines soient terminés et collecter les résultats\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        try:\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Exception occurred: {e}\")\n",
    "\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results : Empty DataFrame\n",
      "Columns: []\n",
      "Index: [CandidatToConcept, SynonymToConcept, AgglomerativeClustering]\n"
     ]
    }
   ],
   "source": [
    "print(\"results :\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception occurred: 'DataFrame' object has no attribute 'append'\n",
      "Exception occurred: 'DataFrame' object has no attribute 'append'\n",
      "Exception occurred: 'DataFrame' object has no attribute 'append'\n",
      "Exception occurred: 'DataFrame' object has no attribute 'append'\n",
      "Exception occurred: 'DataFrame' object has no attribute 'append'\n",
      "Exception occurred: 'DataFrame' object has no attribute 'append'\n",
      "Exception occurred: 'DataFrame' object has no attribute 'append'\n",
      "Exception occurred: 'DataFrame' object has no attribute 'append'\n",
      "Exception occurred: 'DataFrame' object has no attribute 'append'\n",
      "Exception occurred: 'DataFrame' object has no attribute 'append'\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "ct_to_concept_pipelines1 = [\n",
    "    Pipeline(\n",
    "        spacy_model=nlp,\n",
    "        pipeline_components=[\n",
    "            TFIDFTermExtraction(\n",
    "                max_term_token_length=3,\n",
    "                candidate_term_threshold=threshold,\n",
    "                cts_post_processing_functions=[cts_post_processing]\n",
    "            ),\n",
    "            CTsToConceptExtraction()\n",
    "        ],\n",
    "        corpus_loader=corpus_loader\n",
    "    ) for threshold in candidate_term_treshold]\n",
    "\n",
    "\n",
    "results = []\n",
    "# Utilisation de ThreadPoolExecutor pour exécuter les pipelines en parallèle\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = [\n",
    "        executor.submit(get_concept_ratio, pipeline, expected_concepts, hg_lm_similaritiry, comparator_args) \n",
    "        for pipeline in ct_to_concept_pipelines1\n",
    "        ]\n",
    "    \n",
    "\n",
    "    # Attendre que tous les pipelines soient terminés et collecter les résultats\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        try:\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Exception occurred: {e}\")\n",
    "\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF Term Extraction and Synonym Concept Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "synonym_concept_pipelines = [\n",
    "    Pipeline(\n",
    "        spacy_model=nlp,\n",
    "        pipeline_components=[\n",
    "           TFIDFTermExtraction(\n",
    "                max_term_token_length=3,\n",
    "                candidate_term_threshold=threshold,\n",
    "                cts_post_processing_functions=[has_not_punct, has_not_bad_pos, has_not_stopword]\n",
    "            ),\n",
    "            SemanticBasedEnrichment(\n",
    "                threshold=.9\n",
    "            ),\n",
    "            SynonymConceptExtraction()\n",
    "        ],\n",
    "        corpus_loader=corpus_loader\n",
    "    ) for threshold in candidate_term_treshold]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results = []\n",
    "# Utilisation de ThreadPoolExecutor pour exécuter les pipelines en parallèle\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = [\n",
    "        executor.submit(get_concept_ratio, pipeline, expected_concepts, hg_lm_similaritiry, comparator_args) \n",
    "        for pipeline in synonym_concept_pipelines\n",
    "        ]\n",
    "    \n",
    "\n",
    "    # Attendre que tous les pipelines soient terminés et collecter les résultats\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        try:\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Exception occurred: {e}\")\n",
    "\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF Term Extraction and Agglomerative clustering Concept Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<olaf.pipeline.pipeline_schema.Pipeline at 0x70fc37fb2950>,\n",
       " <olaf.pipeline.pipeline_schema.Pipeline at 0x70fc37fb32e0>,\n",
       " <olaf.pipeline.pipeline_schema.Pipeline at 0x70fb784c8250>,\n",
       " <olaf.pipeline.pipeline_schema.Pipeline at 0x70fb784c8e50>,\n",
       " <olaf.pipeline.pipeline_schema.Pipeline at 0x70fb784cb700>,\n",
       " <olaf.pipeline.pipeline_schema.Pipeline at 0x70fc37fd71f0>,\n",
       " <olaf.pipeline.pipeline_schema.Pipeline at 0x70fc37fd6710>,\n",
       " <olaf.pipeline.pipeline_schema.Pipeline at 0x70fc37fd4400>,\n",
       " <olaf.pipeline.pipeline_schema.Pipeline at 0x70fc37fd5ff0>,\n",
       " <olaf.pipeline.pipeline_schema.Pipeline at 0x70fc37fd7f40>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agglo_clustering_pipelines = [\n",
    "     Pipeline(\n",
    "        spacy_model=nlp,\n",
    "        pipeline_components=[\n",
    "            TFIDFTermExtraction(\n",
    "                max_term_token_length=3,\n",
    "                candidate_term_threshold=threshold,\n",
    "                cts_post_processing_functions=[has_not_punct, has_not_bad_pos, has_not_stopword]\n",
    "            ),\n",
    "            AgglomerativeClusteringConceptExtraction(\n",
    "                distance_threshold=.3\n",
    "            )\n",
    "        ],\n",
    "        corpus_loader=corpus_loader\n",
    "    ) for threshold in candidate_term_treshold]\n",
    "\n",
    "results = []\n",
    "# Utilisation de ThreadPoolExecutor pour exécuter les pipelines en parallèle\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = [\n",
    "        executor.submit(get_concept_ratio, pipeline, expected_concepts, hg_lm_similaritiry, comparator_args) \n",
    "        for pipeline in agglo_clustering_pipelines\n",
    "        ]\n",
    "    \n",
    "\n",
    "    # Attendre que tous les pipelines soient terminés et collecter les résultats\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        try:\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Exception occurred: {e}\")\n",
    "\n",
    "for result in results:\n",
    "    print(result)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores des pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation des Resultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour créer les diagrammes en barres\n",
    "def create_bar_chart(index_name, data):\n",
    "    fig, ax = plt.subplots(figsize=(9, 5))\n",
    "    bar_width = 0.2\n",
    "    opacity = 0.8\n",
    "\n",
    "    # Configurer les positions des barres\n",
    "    r1 = np.arange(len(data.columns.levels[0]))\n",
    "    r2 = [x + bar_width for x in r1]\n",
    "    r3 = [x + bar_width for x in r2]\n",
    "\n",
    "    precision = data.loc[index_name].xs('Precision', level=1)\n",
    "    rappel = data.loc[index_name].xs('Rappel', level=1)\n",
    "    f1 = data.loc[index_name].xs('F1', level=1)\n",
    "\n",
    "    rects1 = ax.bar(r1, precision, bar_width, alpha=opacity, color='b', label='Précision')\n",
    "    rects2 = ax.bar(r2, rappel, bar_width, alpha=opacity, color='g', label='Rappel')\n",
    "    rects3 = ax.bar(r3, f1, bar_width, alpha=opacity, color='r', label='F1')\n",
    "\n",
    "    ax.set_xlabel('Composants')\n",
    "    ax.set_ylabel('Scores')\n",
    "    ax.set_title(f'Scores de Précision, Rappel et F1 pour {index_name}')\n",
    "    ax.set_xticks([r + bar_width for r in range(len(data.columns.levels[0]))])\n",
    "    ax.set_xticklabels(data.columns.levels[0])\n",
    "    ax.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Créer un diagramme en barres pour chaque index\n",
    "# for idx in term_extraction_components:\n",
    "#     create_bar_chart(idx, pipelines_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bar_chart(index_name, data):\n",
    "    precision = data.loc[index_name].xs('Precision', level=1)\n",
    "    rappel = data.loc[index_name].xs('Rappel', level=1)\n",
    "    f1 = data.loc[index_name].xs('F1', level=1)\n",
    "\n",
    "    fig = go.Figure(data=[\n",
    "        go.Bar(name='Précision', x=data.columns.levels[0], y=precision),\n",
    "        go.Bar(name='Rappel', x=data.columns.levels[0], y=rappel),\n",
    "        go.Bar(name='F1', x=data.columns.levels[0], y=f1)\n",
    "    ])\n",
    "    \n",
    "    # Modifier la disposition du graphique\n",
    "    fig.update_layout(\n",
    "        title=f'Scores de Précision, Rappel et F1 pour {index_name}',\n",
    "        xaxis_title='Composants',\n",
    "        yaxis_title='Scores',\n",
    "        barmode='group'\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "data_long = pipelines_scores.reset_index().melt(id_vars='index', var_name=['Composant', 'Métrique'], value_name='Score')\n",
    "data_long.rename(columns={'index': 'Extraction'}, inplace=True)\n",
    "\n",
    "def create_bar_chart(index_name, data):\n",
    "    df = data[data['Extraction'] == index_name]\n",
    "    fig = px.bar(df, x='Composant', y='Score', color='Métrique', barmode='group',\n",
    "                 title=f'Scores de Précision, Rappel et F1 pour {index_name}')\n",
    "    \n",
    "    fig.update_layout(\n",
    "        xaxis_title='Composants',\n",
    "        yaxis_title='Scores'\n",
    "    )\n",
    "    fig.update_layout(width=1000, height=600)\n",
    "    fig.show()\n",
    "\n",
    "# # Créer un diagramme en barres pour chaque index\n",
    "# for idx in term_extraction_components:\n",
    "#     create_bar_chart(idx, data_long)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
