{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing C/NC value\n",
    "\n",
    "Method implemented based on <https://doi.org/10.1007/s007999900023>\n",
    "\n",
    "The C-value approach combines linguistic and statistical information, emphasis being placed on the statistical part. The linguistic information consists of the part-of-speech tagging of the corpus, the linguistic filter constraining the type of terms extracted, and the stop-list. The statistical part combines statistical features of the candidate string, in a form of measure that is also called C-value.\n",
    "\n",
    "$ C-value(a) = log_2 |a| \\times f(a) $ if a is not nested,\n",
    "\n",
    "$ C-value(a) = log_2 |a| \\times (f(a)âˆ’ \\frac{1}{P(T_a)} \\sum_{b \\in T_a} f(b)) $ otherwise\n",
    "\n",
    "Section 2.3 The Algorithm, page 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from random import sample\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "from string import punctuation\n",
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "from pprint import pprint as pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"C:/Users/msesboue/OneDrive - TRACEPARTS/These_RESPONDING/Data/es_data_info/schneider_pn_texts/\"\n",
    "# data_file = \"schneider_pn_fr_texts.csv\"\n",
    "data_file = \"schneider_texts.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57395\n"
     ]
    }
   ],
   "source": [
    "with open(data_path + data_file, \"r\", encoding='utf8') as file:\n",
    "    texts = list(set([text.strip('\"\\n') for text in file.readlines()]))\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 10000\n",
    "sample_docs = [txt.lower().split() for txt in sample(texts, sample_size)]\n",
    "# sample(sample_docs, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "This preprocessing step is very specific to my data.\n",
    "\n",
    "TODO:\n",
    "\n",
    "- When generating tokens. If some tokens have been removed in between two sequences, no ngram should be composed of elements of those two separate sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_letters_pattern = re.compile(r'[a-z]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [[token.strip(punctuation) for token in doc] for doc in sample_docs]\n",
    "docs = [[token for token in doc if (len(token) > 0)] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = dict()\n",
    "for i, txt in enumerate(docs):\n",
    "    corpus[i] = txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter the type of terms\n",
    "\n",
    "add filter by occurence ?\n",
    "\n",
    "*Step 1 : We tag the corpus. As mentioned earlier, we need the tagging process since we will use a linguistic filter to restrict the type of terms to be extracted.*\n",
    "\n",
    "*Step 2: This stage extracts those strings that satisfy the linguistic filter and occurence threshold.*\n",
    "\n",
    "In this case we keep tokens composed of only letters and remove stop words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_letters_pattern = re.compile(r'[a-z]+')\n",
    "en_stopwords = stopwords.words('english')\n",
    "\n",
    "empty_docs = []\n",
    "\n",
    "for idx, tokens in corpus.items():\n",
    "    corpus[idx] = [t for t in tokens if (only_letters_pattern.fullmatch(t) is not None) and (t not in en_stopwords)]\n",
    "    \n",
    "    if len(corpus[idx]) == 0:     \n",
    "        empty_docs.append(idx)\n",
    "        \n",
    "for idx in empty_docs:\n",
    "    del corpus[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create N-grams\n",
    "\n",
    "*The lists are then filtered through the stop-list and are concatenated. The longest strings appear at the top, and decrease in size as we move down, with the bigrams being at the bottom. The strings of each length are ordered by their occurrence.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_size_gram = 5 # need to be chosen\n",
    "\n",
    "grams_by_size = defaultdict(list)\n",
    "all_ngrams = []\n",
    "\n",
    "for size in range(1, max_size_gram + 1):\n",
    "    for tokens in corpus.values():\n",
    "        grams_by_size[size].extend([\" \".join(gram) for gram in ngrams(tokens, size)])\n",
    "    \n",
    "    all_ngrams.extend(grams_by_size[size])\n",
    "    # we need all ngrams (with duplicates) only to extract the occurence\n",
    "    # for the we will keep a list of unique ngram instances\n",
    "    grams_by_size[size] = list(set(grams_by_size[size])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_occurences = Counter(all_ngrams) # The occurence of each ngram in the corpus\n",
    "\n",
    "# each group of ngram needs to be ordered by the occurence\n",
    "for size in grams_by_size.keys():\n",
    "    grams_by_size[size].sort(key=lambda ngram: ngram_occurences[ngram], reverse=True)\n",
    "    \n",
    "unique_ngrams = []\n",
    "for size in range(1, max_size_gram + 1).__reversed__():\n",
    "    unique_ngrams.extend(grams_by_size[size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute C-value\n",
    "\n",
    "In the paper, there is mistake in the last if condition alignement. It should be out of the above else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_substrings(string):\n",
    "    tokens = string.split()\n",
    "    token_len = len(tokens)\n",
    "    \n",
    "    substrings = set()\n",
    "    for i in range(1, token_len):\n",
    "        for gram in ngrams(tokens, i):\n",
    "            substrings.add(\" \".join(gram))\n",
    "    \n",
    "    return list(substrings)\n",
    "\n",
    "def update_stat_triple(substring, stat_triples, parent_str, counter):\n",
    "    if substring in stat_triples.keys():\n",
    "        if parent_str in stat_triples.keys():\n",
    "            stat_triples[substring][1] = stat_triples[substring][1] + (counter[parent_str] - stat_triples[parent_str][1])\n",
    "        else:\n",
    "            stat_triples[substring][1] = stat_triples[substring][1] + counter[parent_str]\n",
    "            \n",
    "        stat_triples[substring][2] += 1\n",
    "    else:\n",
    "        f_string = 0 if counter.get(substring) is None else counter[substring]\n",
    "        stat_triples[substring] = [f_string, counter[parent_str], 1]\n",
    "        \n",
    "def process_substrings(ngram, stat_triples, counter):\n",
    "    substrings = get_substrings(ngram)\n",
    "    for substring in substrings:\n",
    "        update_stat_triple(substring, stat_triples, ngram, counter)\n",
    "        \n",
    "def computes_c_values(ngrams, counter, max_size_gram):\n",
    "    \n",
    "    c_values = list()\n",
    "    stat_triples = dict()\n",
    "\n",
    "    for ngram in ngrams:\n",
    "        \n",
    "        len_ngram = len(ngram.split())\n",
    "        \n",
    "        if len_ngram == max_size_gram:\n",
    "            c_val = math.log2(len_ngram) * counter[ngram]\n",
    "            c_values.append((c_val, ngram))\n",
    "        \n",
    "            process_substrings(ngram, stat_triples, counter)\n",
    "        \n",
    "        else:\n",
    "            if ngram not in stat_triples.keys():\n",
    "                c_val = math.log2(len_ngram) * counter[ngram]\n",
    "                c_values.append((c_val, ngram))\n",
    "            else:\n",
    "                c_val = math.log2(len_ngram) * (counter[ngram] - (stat_triples[ngram][1] / stat_triples[ngram][2]))\n",
    "                c_values.append((c_val, ngram))\n",
    "                \n",
    "            process_substrings(ngram, stat_triples, counter)\n",
    "    \n",
    "    return c_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_values = computes_c_values(unique_ngrams, ngram_occurences, 5)\n",
    "\n",
    "c_values.sort(key=lambda c_val: c_val[0], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watch results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4260.738054118309, 'kw power base control unit'),\n",
       " (4228.231060789886, 'base control unit communication module'),\n",
       " (4228.231060789886, 'power base control unit communication'),\n",
       " (3139.2467842877136, 'control unit communication module accessories'),\n",
       " (2492.9195904199896, 'base control unit'),\n",
       " (2326.724951058657, 'power base control'),\n",
       " (2308.97337105058, 'control unit communication'),\n",
       " (2164.662535359919, 'unit communication module'),\n",
       " (1797.5294117647059, 'control unit'),\n",
       " (1722.6666666666667, 'communication module')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_values[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "Example from the paper. \n",
    "\n",
    "| C-value | P(Ta) | SUM(f(b)) | Freq. | Candidate Terms |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| 11.6096 | 0 | 0 | 5 | ADENOID CYSTIC BASAL CELL CARCINOMA |\n",
    "| 12 | 5 | 1 | 11 | CYSTIC BASAL CELL CARCINOMA |\n",
    "| 14 | 0 | 0 | 7 | ULCERATED BASAL CELL CARCINOMA |\n",
    "| 10 | 0 | 0 | 5 | RECURRENT BASAL CELL CARCINOMA |\n",
    "| 6 | 0 | 0 | 3 | CIRCUMSCRIBED BASAL CELL CARCINOMA |\n",
    "| 1551.36 | 26 | 5 | 984 | BASAL CELL CARCINOMA |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(11.60964047443681, 'ADENOID CYSTIC BASAL CELL CARCINOMA'),\n",
      " (12.0, 'CYSTIC BASAL CELL CARCINOMA'),\n",
      " (14.0, 'ULCERATED BASAL CELL CARCINOMA'),\n",
      " (10.0, 'RECURRENT BASAL CELL CARCINOMA'),\n",
      " (6.0, 'CIRCUMSCRIBED BASAL CELL CARCINOMA'),\n",
      " (1551.3612957058674, 'BASAL CELL CARCINOMA')]\n"
     ]
    }
   ],
   "source": [
    "test_ngrams = [\n",
    "    \"ADENOID CYSTIC BASAL CELL CARCINOMA\",\n",
    "    \"CYSTIC BASAL CELL CARCINOMA\",\n",
    "    \"ULCERATED BASAL CELL CARCINOMA\",\n",
    "    \"RECURRENT BASAL CELL CARCINOMA\",\n",
    "    \"CIRCUMSCRIBED BASAL CELL CARCINOMA\",\n",
    "    \"BASAL CELL CARCINOMA\"\n",
    "]\n",
    "\n",
    "test_counter = {\n",
    "    \"ADENOID CYSTIC BASAL CELL CARCINOMA\": 5,\n",
    "    \"CYSTIC BASAL CELL CARCINOMA\": 11,\n",
    "    \"ULCERATED BASAL CELL CARCINOMA\": 7,\n",
    "    \"RECURRENT BASAL CELL CARCINOMA\": 5,\n",
    "    \"CIRCUMSCRIBED BASAL CELL CARCINOMA\": 3,\n",
    "    \"BASAL CELL CARCINOMA\": 984\n",
    "}\n",
    "\n",
    "\n",
    "test_c_values = computes_c_values(test_ngrams, test_counter, 5)          \n",
    "\n",
    "pp(test_c_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9fa8db58f91cbab1acd722ad9914077389bbed75cb0ee4cdd31751f68f9af9dc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
