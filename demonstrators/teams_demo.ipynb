{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLAF : Microsoft Teams Pipeline\n",
    "\n",
    "In this demo, we create a pipeline using components from the OLAF library. We use the folder (../teams_doc_250/) and we preprocess the data by filtering out stopwords, punctuation, numbers and URLs, and extract the following components for the pipeline : term extraction (nouns), term enrichment (by their synonyms), concept extractions and hierarchisation, and relation extraction (based off verbs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-15 11:04:11,756] [WARNING] [token_selector_data_preprocessing] [__post_init__] [Data preprocessing token sequence attribute not set by the user. \n",
      "                By default the token sequence attribute selected_tokens will be used.]\n",
      "/home/mcg/Desktop/GM4/GM4-2.0/ontology-learning/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import all necessary items from the olaf package\n",
    "from olaf import Pipeline\n",
    "from olaf.pipeline.data_preprocessing import TokenSelectorDataPreprocessing, TokenSelectorDataPreprocessingConfig #Delete TokenSelectorDataPreprocessingConfig after the merge\n",
    "from olaf.commons.spacy_processing_tools import is_not_stopword, is_not_punct, is_not_num, is_not_url\n",
    "from olaf.repository.corpus_loader import JsonCorpusLoader\n",
    "from olaf.pipeline.pipeline_component.term_extraction import POSTermExtraction\n",
    "from olaf.pipeline.data_preprocessing import TokenSelectorDataPreprocessing \n",
    "from olaf.pipeline.pipeline_component.concept_relation_extraction import CTsToRelationExtraction, CTsToConceptExtraction\n",
    "from olaf.pipeline.pipeline_component.candidate_term_enrichment import KnowledgeBasedCTermEnrichment\n",
    "from olaf.pipeline.pipeline_component.concept_relation_hierarchy import SubsumptionHierarchisation\n",
    "from olaf.pipeline.pipeline_component.concept_relation_extraction import SynonymConceptExtraction\n",
    "from olaf.repository.serialiser import BaseOWLSerialiser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load the language model according to our needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spacy language model according to the corpus (needs to be downloaded in the virtual environnement)\n",
    "spacy_model = spacy.load(\"fr_core_news_sm\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOADING DATA\n",
    "\n",
    "To load the data, we use the JsonCorpusLoader that will go in the folder and read through all necessary files and add them to the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an instance of JsonCorpusLoader with the path to the JSON files and the field name wanted\n",
    "corpus_loader = JsonCorpusLoader(corpus_path=\"../data/teams_doc_250/\", json_field=\"description\")\n",
    "\n",
    "# Load the data using JsonCorpusLoader\n",
    "text_corpus = corpus_loader._read_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 241 documents in our corpus but we will only use the first 10 for this pipeline. So that is what we are doing in the following section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(text_corpus))\n",
    "partial_corpus=[doc for doc in spacy_model.pipe(text_corpus[0:10])]\n",
    "print(len(partial_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA PREPROCESSING\n",
    "\n",
    "Starting by preprocessing the data, and filtering out all stopwrds, punctuation, numbers and urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-15 11:04:28,946] [WARNING] [token_selector_data_preprocessing] [__post_init__] [Data preprocessing token sequence attribute not set by the user. \n",
      "                By default the token sequence attribute selected_tokens will be used.]\n"
     ]
    }
   ],
   "source": [
    "token_selector_params = {\n",
    "    \"selectors\": [is_not_num, is_not_url, is_not_punct, is_not_stopword],\n",
    "    \"token_sequence_doc_attribute\": \"selected_tokens\"\n",
    "}\n",
    "\n",
    "# To be modified with the updated TokenSelectorDataPreprocessing\n",
    "default_config = TokenSelectorDataPreprocessingConfig()\n",
    "\n",
    "# Creating a list of preprocessing components\n",
    "data_prep = [TokenSelectorDataPreprocessing(\n",
    "    selector=lambda token: all(selector(token) for selector in token_selector_params[\"selectors\"]), config=default_config\n",
    ")]\n",
    "\n",
    "\n",
    "# Creating a list of preprocessing components After the merge\n",
    "#data_prep = [TokenSelectorDataPreprocessing(\n",
    "#    selector=lambda token: all(selector(token) for selector in token_selector_params[\"selectors\"])\n",
    "#)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TERM EXTRACTION\n",
    "Next, extracting candidates terms based on POS tagging (taking NOUNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-15 11:04:32,034] [WARNING] [pos_term_extraction] [__init__] [No preprocessing function provided for tokens. Using the default one.]\n"
     ]
    }
   ],
   "source": [
    "term_extract_params_concepts = {\n",
    "    \"token_sequence_doc_attribute\": \"selected_tokens\",\n",
    "    \"pos_selection\": [\"NOUN\"]\n",
    "}\n",
    "\n",
    "concept_term_extraction = POSTermExtraction(parameters=term_extract_params_concepts)\n",
    "concept_extraction = CTsToConceptExtraction(parameters={\"concept_max_distance\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RELATION EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-15 11:04:34,168] [WARNING] [pos_term_extraction] [__init__] [No preprocessing function provided for tokens. Using the default one.]\n"
     ]
    }
   ],
   "source": [
    "term_extract_params_relations = {\n",
    "    \"token_sequence_doc_attribute\": \"selected_tokens\",\n",
    "    \"pos_selection\": [\"VERB\"],\n",
    "}\n",
    "\n",
    "relation_term_extraction = POSTermExtraction(parameters=term_extract_params_relations, )\n",
    "relation_extraction = CTsToRelationExtraction(parameters={\"concept_max_distance\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIPELINE SETUP\n",
    "\n",
    "We can now create a pipeline with the components we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the object Pipeline with all the components instanciated above\n",
    "teams_demo_pipeline = Pipeline(\n",
    "    spacy_model=spacy_model,\n",
    "    preprocessing_components=data_prep,\n",
    "    pipeline_components=[\n",
    "        concept_term_extraction,\n",
    "        concept_extraction\n",
    "    ],\n",
    "    corpus=[doc for doc in spacy_model.pipe(partial_corpus)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other components created can also be added to the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_demo_pipeline.add_pipeline_component(relation_term_extraction)\n",
    "teams_demo_pipeline.add_pipeline_component(relation_extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KnowledgeRepresentation(concepts=set(), relations=set(), metarelations=set())"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Empty list   \n",
    "teams_demo_pipeline.kr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'spacy.tokens.token.Token' object has no attribute 'start'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/mcg/Desktop/GM4/GM4-2.0/ontology-learning/demonstrators/teams_demo.ipynb Cell 21\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/mcg/Desktop/GM4/GM4-2.0/ontology-learning/demonstrators/teams_demo.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Running pipeline created \u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/mcg/Desktop/GM4/GM4-2.0/ontology-learning/demonstrators/teams_demo.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m teams_demo_pipeline\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/Desktop/GM4/GM4-2.0/ontology-learning/olaf/pipeline/pipeline_schema.py:145\u001b[0m, in \u001b[0;36mPipeline.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    142\u001b[0m     component\u001b[39m.\u001b[39mrun(\u001b[39mself\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[39mfor\u001b[39;00m component \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpipeline_components :\n\u001b[0;32m--> 145\u001b[0m     component\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/Desktop/GM4/GM4-2.0/ontology-learning/olaf/pipeline/pipeline_component/concept_relation_extraction/candidate_terms_to_relations.py:103\u001b[0m, in \u001b[0;36mCTsToRelationExtraction.run\u001b[0;34m(self, pipeline)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39mfor\u001b[39;00m concept \u001b[39min\u001b[39;00m pipeline\u001b[39m.\u001b[39mkr\u001b[39m.\u001b[39mconcepts:\n\u001b[1;32m    101\u001b[0m     concepts_labels_map[concept\u001b[39m.\u001b[39mlabel] \u001b[39m=\u001b[39m concept\n\u001b[0;32m--> 103\u001b[0m candidate_relations \u001b[39m=\u001b[39m cts_to_crs(\n\u001b[1;32m    104\u001b[0m     pipeline\u001b[39m.\u001b[39;49mcandidate_terms,\n\u001b[1;32m    105\u001b[0m     concepts_labels_map,\n\u001b[1;32m    106\u001b[0m     pipeline\u001b[39m.\u001b[39;49mspacy_model,\n\u001b[1;32m    107\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconcept_max_distance,\n\u001b[1;32m    108\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscope,\n\u001b[1;32m    109\u001b[0m )\n\u001b[1;32m    111\u001b[0m \u001b[39mfor\u001b[39;00m cr \u001b[39min\u001b[39;00m candidate_relations:\n\u001b[1;32m    112\u001b[0m     pipeline\u001b[39m.\u001b[39mkr\u001b[39m.\u001b[39mrelations\u001b[39m.\u001b[39madd(crs_to_relation({cr}))\n",
      "File \u001b[0;32m~/Desktop/GM4/GM4-2.0/ontology-learning/olaf/commons/relation_tools.py:93\u001b[0m, in \u001b[0;36mcts_to_crs\u001b[0;34m(candidate_terms, concepts_labels_map, spacy_model, concept_max_distance, scope)\u001b[0m\n\u001b[1;32m     89\u001b[0m destination_concepts \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[1;32m     91\u001b[0m \u001b[39mfor\u001b[39;00m match_id, start, end \u001b[39min\u001b[39;00m matches:\n\u001b[1;32m     92\u001b[0m     \u001b[39m# Check for source concept\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m     \u001b[39mif\u001b[39;00m (start \u001b[39m<\u001b[39m co\u001b[39m.\u001b[39;49mstart) \u001b[39mand\u001b[39;00m (\n\u001b[1;32m     94\u001b[0m         start \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39m0\u001b[39m, co\u001b[39m.\u001b[39mstart \u001b[39m-\u001b[39m concept_max_distance)\n\u001b[1;32m     95\u001b[0m     ):\n\u001b[1;32m     96\u001b[0m         source_concepts\u001b[39m.\u001b[39madd(\n\u001b[1;32m     97\u001b[0m             concepts_labels_map\u001b[39m.\u001b[39mget(spacy_model\u001b[39m.\u001b[39mvocab\u001b[39m.\u001b[39mstrings[match_id])\n\u001b[1;32m     98\u001b[0m         )\n\u001b[1;32m    100\u001b[0m     \u001b[39m# Check for destination concept\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'spacy.tokens.token.Token' object has no attribute 'start'"
     ]
    }
   ],
   "source": [
    "# Running pipeline created \n",
    "teams_demo_pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now has concepts\n",
    "teams_demo_pipeline.kr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No concepts found\n",
    "# Print out the concepts found\n",
    "for concept in teams_demo_pipeline.kr.concepts:\n",
    "    print(concept.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the relations found\n",
    "for relation in teams_demo_pipeline.kr.relations:\n",
    "    print(relation.label)\n",
    "\n",
    "# REMARQUE : le preprocessing n'a pas été pris en compte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_candidate_terms = concept_term_extraction._extract_candidate_tokens(token_sequences=teams_demo_pipeline.corpus)\n",
    "occurence_candidate_terms = concept_term_extraction._build_term_corpus_occ_map(pos_candidate_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(teams_demo_pipeline.candidate_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove wrong candidate terms\n",
    "candidate_terms_to_remove = [\"cc\", \"pouvoir\", \"bit\", \">\", \"<\", \"oui\"]\n",
    "\n",
    "# Merge candidate terms\n",
    "candidate_terms = [candidate_term for candidate_term in pos_candidate_terms if candidate_term in occurence_candidate_terms and candidate_term not in candidate_terms_to_remove]\n",
    "\n",
    "# Print number of candidates term found\n",
    "print(f\"{len(candidate_terms)} candidate terms have been found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TERM ENRICHMENT\n",
    "Embedding-based similar term extraction. \n",
    "\n",
    "We can add the components we create in the following sections to the pipeline we created above and by running the pipeline again, we will be able to have the candidate terms and concepts and relations we are searching for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_enrichment = KnowledgeBasedCTermEnrichment(teams_demo_pipeline.candidate_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_demo_pipeline.add_pipeline_component(term_enrichment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONCEPT EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_extraction = SynonymConceptExtraction(teams_demo_pipeline.candidate_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_demo_pipeline.add_pipeline_component(concept_extraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONCEPT HIERARCHY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_hierarchy = SubsumptionHierarchisation()\n",
    "concept_hierarchy._is_sub_hierarchy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_demo_pipeline.add_pipeline_component(concept_hierarchy)\n",
    "teams_demo_pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now has concepts\n",
    "teams_demo_pipeline.kr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the concepts found\n",
    "for concept in teams_demo_pipeline.kr.concepts:\n",
    "    print(concept.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SERIALISER\n",
    "\n",
    "To save the results of this pipeline, we use a serialiser to export the results in turtle (\".ttl\") format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating serialiser\n",
    "teams_kr_serialiser = BaseOWLSerialiser(\"http://teams_kr.org/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the RDF graph from the olaf pipeline KnowledgeRepresentation\n",
    "teams_kr_serialiser.build_graph(teams_demo_pipeline.kr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the RDF graph file path and in default format (turtle)\n",
    "teams_kr_serialiser.export_graph(\"teams_kr.ttl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
